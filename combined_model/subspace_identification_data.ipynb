{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read config file\n",
    "import yaml\n",
    "\n",
    "def read_config_file(config_file):\n",
    "    with open(config_file, 'r') as stream:\n",
    "        try:\n",
    "            config = yaml.safe_load(stream)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = '/home/shiqi/code/Project2-sensor-case/model_combination_Argos/combined_model_20240805/outputs/experiment_1/config.yaml'\n",
    "config = read_config_file(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/shiqi/code/Project2-sensor-case/model_combination_Argos/utils')\n",
    "from load_dataset import cut_slices, load_dataset\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def data_preparation(config, data_dir):\n",
    "    window_size = config['window_size']\n",
    "    print(f'window_size: {window_size}')\n",
    "    x_dataset, y_dataset, u_dataset = [], [], []\n",
    "    # Load data\n",
    "    for item in os.listdir(data_dir):\n",
    "        data_file_path = os.path.join(data_dir, item)\n",
    "\n",
    "        # Check if the file exists before trying to load it\n",
    "        \n",
    "        if os.path.exists(data_file_path) and data_file_path.endswith('.npy'):\n",
    "            \n",
    "            data_dict = np.load(data_file_path, allow_pickle=True).item()\n",
    "            x_data, y_data, u_data, _ = load_dataset(data_dict)\n",
    "            # print(x_data.shape, y_data.shape, u_data.shape)\n",
    "            x_dataset.append(x_data[1:window_size])\n",
    "            y_dataset.append(y_data[1:window_size])\n",
    "            u_dataset.append(u_data[1:window_size])\n",
    "            # print(f\"Loaded data from {data_file_path}\")\n",
    "        else:\n",
    "            print(f\"File not found: {data_file_path}\")\n",
    "\n",
    "    print(x_dataset[0].shape, y_dataset[0].shape, u_dataset[0].shape)\n",
    "    # Concatenate data\n",
    "    x_data = np.concatenate(x_dataset, axis=0)\n",
    "    y_data = np.concatenate(y_dataset, axis=0)\n",
    "    u_data = np.concatenate(u_dataset, axis=0)\n",
    "    print(f'x_data shape: {x_data.shape}, y_data shape: {y_data.shape}, u_data shape: {u_data.shape}')\n",
    "\n",
    "    return x_data, y_data, u_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class StdScalerLayer(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super(StdScalerLayer, self).__init__()\n",
    "        if not isinstance(mean, torch.Tensor):\n",
    "            mean = torch.tensor(mean, dtype=torch.float32)\n",
    "        if not isinstance(std, torch.Tensor):\n",
    "            std = torch.tensor(std, dtype=torch.float32)\n",
    "        self.mean = nn.Parameter(mean, requires_grad=False)\n",
    "        self.std = nn.Parameter(std, requires_grad=False)\n",
    "\n",
    "    def transform(self, x):\n",
    "        return (x - self.mean) / self.std\n",
    "    \n",
    "    def inverse_transform(self, input):\n",
    "        return input * self.std + self.mean\n",
    "    \n",
    "class Linear_model(torch.nn.Module):\n",
    "    def __init__(self, state_dim, control_dim):\n",
    "        super(Linear_model, self).__init__()\n",
    "        self.A = torch.nn.Parameter(torch.randn(state_dim, state_dim))\n",
    "\n",
    "        self.B = torch.nn.Parameter(torch.randn(control_dim, state_dim))\n",
    "    \n",
    "    def x_dict(self, x):\n",
    "        ones = torch.ones(x.shape[0], 1).to(x.device)\n",
    "        return torch.cat((x, ones), dim=1)\n",
    "    \n",
    "    def u_dict(self, u):\n",
    "        return u\n",
    "    \n",
    "    def forward(self, x, u):\n",
    "        x = self.x_dict(x)\n",
    "        u = self.u_dict(u)\n",
    "        y = torch.matmul(x, self.A) + torch.matmul(u, self.B)\n",
    "        return y[:, 1:]\n",
    "    \n",
    "class PCALayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, pca_matrix):\n",
    "        super(PCALayer, self).__init__()\n",
    "        self.pca_matrix = pca_matrix\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.transform = nn.Linear(input_dim, output_dim, bias = False)\n",
    "        self.transform.weight = nn.Parameter(pca_matrix, requires_grad=False)\n",
    "        self.inverse_transform = nn.Linear(output_dim, input_dim, bias = False)\n",
    "        self.inverse_transform.weight = nn.Parameter(pca_matrix.T, requires_grad=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size: 150\n",
      "(149, 6957) (149, 6957) (149, 2)\n",
      "x_data shape: (6258, 6957), y_data shape: (6258, 6957), u_data shape: (6258, 2)\n",
      "PCA matrix shape: torch.Size([4, 6957])\n",
      "PCA data shape: torch.Size([6258, 4]), torch.Size([6258, 4]), torch.Size([6258, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cpu')\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load data\n",
    "x_data, y_data, u_data = data_preparation(config, config['train_data_dir'])\n",
    "x_data = torch.tensor(x_data, dtype=torch.float32).to(device)\n",
    "y_data = torch.tensor(y_data, dtype=torch.float32).to(device)\n",
    "u_data = torch.tensor(u_data, dtype=torch.float32).to(device)\n",
    "\n",
    "x_dim = x_data.shape[-1]\n",
    "u_dim = u_data.shape[-1]\n",
    "\n",
    "## PCA\n",
    "# Standardize data\n",
    "mean_1 = torch.mean(x_data, dim=0)\n",
    "std_1 = torch.std(x_data, dim=0)\n",
    "std_layer_1 = StdScalerLayer(mean_1, std_1)\n",
    "x_data_scaled = std_layer_1.transform(x_data)\n",
    "\n",
    "# PCA layer\n",
    "pca = PCA(n_components=config['pca_dim'])\n",
    "# Ensure x_data_scaled is converted back to a NumPy array for PCA\n",
    "pca.fit(x_data_scaled.detach().cpu().numpy())\n",
    "components = pca.components_\n",
    "pca_matrix = torch.tensor(components, dtype=torch.float32).to(device)\n",
    "print(f'PCA matrix shape: {pca_matrix.shape}')\n",
    "pca_layer = PCALayer(x_dim, config['pca_dim'], pca_matrix)\n",
    "\n",
    "# Standardize data 2\n",
    "x_pca = pca_layer.transform(x_data_scaled)\n",
    "mean_2 = torch.mean(x_pca, dim=0)\n",
    "std_2 = torch.std(x_pca, dim=0)\n",
    "std_layer_2 = StdScalerLayer(mean_2, std_2)\n",
    "\n",
    "# Build pca dataset\n",
    "x_pca_scaled = std_layer_2.transform(x_pca)\n",
    "y_data_scaled = std_layer_1.transform(y_data)\n",
    "y_pca = pca_layer.transform(y_data_scaled)\n",
    "y_pca_scaled = std_layer_2.transform(y_pca)\n",
    "mean_u = torch.mean(u_data, dim=0)\n",
    "std_u = torch.std(u_data, dim=0)\n",
    "std_layer_u = StdScalerLayer(mean_u, std_u)\n",
    "u_data_scaled = std_layer_u.transform(u_data)\n",
    "dataset = [x_pca_scaled, y_pca_scaled, u_data_scaled]\n",
    "print(f'PCA data shape: {x_pca_scaled.shape}, {y_pca_scaled.shape}, {u_data_scaled.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def Build_Hankel_Matrix(y, i, j):\n",
    "    \"\"\"\n",
    "    Construct the Hankel matrix H_y from y(t).\n",
    "    \"\"\"\n",
    "    y_dim = y.shape[0]\n",
    "    T = y.shape[1]\n",
    "    H_y = torch.zeros(2*i*y_dim, j)\n",
    "    if 2 * i + j > T:\n",
    "        raise ValueError(\"The given i and j are too large for the given data.\")\n",
    "    for k in range(2 * i):\n",
    "        H_y[k*y_dim:(k+1)*y_dim, :] = y[:, k:k+j]\n",
    "    return H_y\n",
    "\n",
    "def H_past_Matrix(H_y, i, y_dim):\n",
    "    \"\"\"\n",
    "    Construct the past matrix H_{y,past} from the Hankel matrix H_y.\n",
    "    \"\"\"\n",
    "    return H_y[:i*y_dim, :]\n",
    "\n",
    "def H_future_Matrix(H_y, i, y_dim):\n",
    "    \"\"\"\n",
    "    Construct the future matrix H_{y,future} from the Hankel matrix H_y.\n",
    "    \"\"\"\n",
    "    return H_y[i*y_dim:2*i*y_dim, :]\n",
    "\n",
    "def H_past_Matrix_plus(H_y, i, y_dim):\n",
    "    \"\"\"\n",
    "    Construct the past matrix H_{y,past}^+ from the Hankel matrix H_y.\n",
    "    \"\"\"\n",
    "    return H_y[:(i+1)*y_dim, :]\n",
    "\n",
    "def H_future_Matrix_minus(H_y, i, y_dim):\n",
    "    \"\"\"\n",
    "    Construct the future matrix H_{y,future}^- from the Hankel matrix H_y.\n",
    "    \"\"\"\n",
    "    return H_y[(i+1)*y_dim:(2*i+1)*y_dim, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def truncated_svd_pinv(matrix, threshold=1e-2):\n",
    "    \"\"\"\n",
    "    Compute the pseudoinverse of a matrix using truncated SVD to improve numerical stability.\n",
    "    \"\"\"\n",
    "    U, S, V = torch.svd(matrix)\n",
    "    \n",
    "    # 只保留大于阈值的奇异值及其对应的U和V\n",
    "    valid_indices = S > threshold\n",
    "    S_truncated = S[valid_indices]\n",
    "    U_truncated = U[:, valid_indices]\n",
    "    V_truncated = V[:, valid_indices]\n",
    "    \n",
    "    # 计算截断后的S_inv\n",
    "    S_inv = torch.diag(1.0 / S_truncated)\n",
    "    \n",
    "    # 计算伪逆矩阵\n",
    "    return V_truncated @ S_inv @ U_truncated.T\n",
    "\n",
    "# Define the Orthogonal and Oblique Projections\n",
    "def Orthogonal_Complement(A):\n",
    "    \"\"\"\n",
    "    Compute the orthogonal complement of matrix A.\n",
    "    \"\"\"\n",
    "    print(\"Condition number of A @ A.T (regularized):\", torch.linalg.cond(A @ A.T + 1e-2 * torch.eye(A.shape[0])))\n",
    "    # print(\"A @ A.T:\", A @ A.T)\n",
    "    return torch.eye(A.shape[1]) - A.T @ truncated_svd_pinv(A @ A.T) @ A\n",
    "\n",
    "def Orthogonal_Projection(A, B):\n",
    "    \"\"\"\n",
    "    Compute the orthogonal projection of the row vectors of matrix A onto the row vectors of matrix B.\n",
    "    \"\"\"\n",
    "    print(\"Condition number of B @ B.T:\", torch.linalg.cond(B @ B.T))\n",
    "    return A @ B.T @ truncated_svd_pinv(B @ B.T) @ B\n",
    "\n",
    "def Oblique_Projection(A, B, C):\n",
    "    \"\"\"\n",
    "    Calculate the oblique projection of matrix A onto the subspace spanned by the row vectors of matrix C with the direction of matrix B.\n",
    "    \"\"\"\n",
    "    B_orth = Orthogonal_Complement(B)\n",
    "    P_ABorth = Orthogonal_Projection(A, B_orth)\n",
    "    P_CBorth = Orthogonal_Projection(C, B_orth)\n",
    "    print(\"Condition number of P_CBorth:\", torch.linalg.cond(P_CBorth))\n",
    "    return P_ABorth @ truncated_svd_pinv(P_CBorth) @ C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Subspace_Identification(y, u, i, j):\n",
    "    H_y = Build_Hankel_Matrix(y, i, j)\n",
    "    H_u = Build_Hankel_Matrix(u, i, j)\n",
    "    Y_p = H_past_Matrix(H_y, i, y.shape[0])\n",
    "    Y_f = H_future_Matrix(H_y, i, y.shape[0])\n",
    "    U_p = H_past_Matrix(H_u, i, u.shape[0])\n",
    "    U_f = H_future_Matrix(H_u, i, u.shape[0])\n",
    "    Y_p_plus = H_past_Matrix_plus(H_y, i, y.shape[0])\n",
    "    Y_f_minus = H_future_Matrix_minus(H_y, i, y.shape[0])\n",
    "    U_p_plus = H_past_Matrix_plus(H_u, i, u.shape[0])\n",
    "    U_f_minus = H_future_Matrix_minus(H_u, i, u.shape[0])\n",
    "    W_p = torch.cat((Y_p, U_p), 0)\n",
    "    W_p_plus = torch.cat((Y_p_plus, U_p_plus), 0)\n",
    "\n",
    "    # Calculate the oblique projection\n",
    "    O_i = Oblique_Projection(Y_f, U_f, W_p)\n",
    "    O_i_minus = Oblique_Projection(Y_f_minus, U_f_minus, W_p_plus)\n",
    "\n",
    "    # Calculate the SVD of the weighted oblique projection\n",
    "    U, S, VT = torch.svd(O_i)\n",
    "    print(O_i.shape)\n",
    "\n",
    "    # Determine the order of the system\n",
    "    threshold = 1e-3\n",
    "    r = torch.sum(S > threshold).item()\n",
    "    U_1 = U[:, :r]\n",
    "    S_1 = torch.diag(S[:r])\n",
    "    print(S)\n",
    "\n",
    "    # Determine Gamma_i and Gamma_{i-1}\n",
    "    Gamma_i = U_1 @ torch.sqrt(S_1)\n",
    "    print(Gamma_i.shape)\n",
    "    Gamma_i_minus = Gamma_i[y.shape[0]:, :]\n",
    "\n",
    "    # Determine X_i and X_{i+1}\n",
    "    X_i = torch.linalg.pinv(Gamma_i) @ O_i\n",
    "    X_i_plus = torch.linalg.pinv(Gamma_i_minus) @ O_i_minus\n",
    "\n",
    "    # Determine the system matrices\n",
    "    Y_i = H_y[i*y.shape[0]:(i+1)*y.shape[0], :]\n",
    "    U_i = H_u[i*u.shape[0]:(i+1)*u.shape[0], :]\n",
    "\n",
    "    X_plus_Y = torch.cat((X_i_plus, Y_i), 0)\n",
    "    X_U = torch.cat((X_i, U_i), 0)\n",
    "\n",
    "    System_Matrices = X_plus_Y @ torch.linalg.pinv(X_U)\n",
    "    A = System_Matrices[:X_i.shape[0], :X_i.shape[0]]\n",
    "    B = System_Matrices[:X_i.shape[0], X_i.shape[0]:]\n",
    "    C = System_Matrices[X_i.shape[0]:, :X_i.shape[0]]\n",
    "    D = System_Matrices[X_i.shape[0]:, X_i.shape[0]:]\n",
    "\n",
    "    return A, B, C, D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition number of A @ A.T (regularized): tensor(6566.5649)\n",
      "Condition number of B @ B.T: tensor(2.0550e+08)\n",
      "Condition number of B @ B.T: tensor(2.0550e+08)\n",
      "Condition number of P_CBorth: tensor(48530760.)\n",
      "Condition number of A @ A.T (regularized): tensor(6220.4907)\n",
      "Condition number of B @ B.T: tensor(1.3532e+08)\n",
      "Condition number of B @ B.T: tensor(1.3532e+08)\n",
      "Condition number of P_CBorth: tensor(55350480.)\n",
      "torch.Size([80, 5])\n",
      "tensor([5.3083e+00, 2.3738e+00, 3.6527e-01, 5.6254e-07, 4.7999e-07])\n",
      "torch.Size([80, 3])\n"
     ]
    }
   ],
   "source": [
    "xx = x_pca_scaled[:config['window_size']-1, :].T\n",
    "uu = u_data_scaled[:config['window_size']-1, :].T\n",
    "A_pred, B_pred, C_pred, D_pred = Subspace_Identification(xx, uu, 20, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 9.8899e-01,  2.1994e-03, -8.1904e-04],\n",
      "        [-1.4702e-02,  1.0030e+00, -9.5919e-04],\n",
      "        [ 1.5825e-02, -3.3876e-03,  1.0011e+00]]) tensor([[ 0.7205, -0.6674],\n",
      "        [ 0.6617, -0.6398],\n",
      "        [-3.7559,  3.2415]]) tensor([[-3.7479e-03, -1.7206e-02,  1.4258e-04],\n",
      "        [-6.1450e-02,  4.5533e-02, -7.1907e-03],\n",
      "        [-1.9295e-01,  5.4494e-01, -9.5441e-02],\n",
      "        [-8.4975e-01, -2.0733e-01, -1.8462e-01]]) tensor([[ -7.3423,   6.4658],\n",
      "        [ -7.0361,   8.3974],\n",
      "        [ 11.5477, -10.5630],\n",
      "        [  4.4944,  -3.2356]])\n"
     ]
    }
   ],
   "source": [
    "print(A_pred, B_pred, C_pred, D_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_linear_trajectories(y_dataset, u_dataset, matrices, std_layer_1, pca_transformer, std_layer_2, std_layer_u, divice = 'cpu'):\n",
    "    A, B, C, D = matrices\n",
    "    x_dim = A.shape[0]\n",
    "    u_dim = B.shape[0]\n",
    "    y_data_pca_pred_traj = []\n",
    "    y_data_pca_traj = []\n",
    "    y_data_pred_traj = []\n",
    "    for y_data, u_data in zip(y_dataset, u_dataset):\n",
    "        y_data = torch.tensor(y_data, dtype=torch.float32).to(device)\n",
    "        u_data = torch.tensor(u_data, dtype=torch.float32).to(device)\n",
    "        y_data_scaled = std_layer_1.transform(y_data)\n",
    "        y_pca = pca_transformer.transform(y_data_scaled)\n",
    "        y_pca_scaled = std_layer_2.transform(y_pca)\n",
    "        y_pred = torch.zeros(y_pca_scaled.shape)\n",
    "        y_pred[0, :] = y_pca_scaled[0, :]\n",
    "\n",
    "        u_data_scaled = std_layer_u.transform(u_data)\n",
    "        y0 = y_pca_scaled[0, :].unsqueeze(0)\n",
    "        x0 = torch.linalg.pinv(C) @ (y0.T - D @ u_data_scaled[0, :].unsqueeze(0).T)\n",
    "        # print(x0.shape)\n",
    "        for i in range(1, y_data_scaled.shape[0]):\n",
    "            x1 = A @ x0 + B @ u_data_scaled[i-1, :].unsqueeze(0).T\n",
    "            y1 = C @ x1 + D @ u_data_scaled[i-1, :].unsqueeze(0).T\n",
    "            x0 = x1\n",
    "            y_pred[i, :] = y1.T\n",
    "        y_data_pca_traj.append(y_pca_scaled.detach().cpu().numpy())\n",
    "        y_data_pca_pred_traj.append(y_pred.detach().cpu().numpy())\n",
    "        y_pca_pred = std_layer_2.inverse_transform(y_pred)\n",
    "        y_data_pred_scaled = pca_transformer.inverse_transform(y_pca_pred)\n",
    "        y_data_pred = std_layer_1.inverse_transform(y_data_pred_scaled)\n",
    "        y_data_pred_traj.append(y_data_pred.detach().cpu().numpy())\n",
    "    \n",
    "    return y_data_pred_traj, y_data_pca_pred_traj, y_data_pca_traj\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 151 /home/shiqi/code/Project2-sensor-case/model_combination_Argos/data_dir/data_20240621\n"
     ]
    }
   ],
   "source": [
    "# Evaluation data\n",
    "### Evaluation\n",
    "def load_evaluation_data(begin, end, data_dir):\n",
    "    x_dataset = []\n",
    "    u_dataset = []\n",
    "\n",
    "    for item in os.listdir(data_dir):\n",
    "        data_file_path = os.path.join(data_dir, item)\n",
    "\n",
    "        # Check if the file exists before trying to load it\n",
    "        if os.path.exists(data_file_path) and item.endswith('.npy'):\n",
    "            data_dict = np.load(data_file_path, allow_pickle=True).item()\n",
    "            x_data, _, u_data, _ = load_dataset(data_dict)\n",
    "            x_dataset.append(x_data[begin:end, :])\n",
    "            u_dataset.append(u_data[begin:end, :])\n",
    "        else:\n",
    "            print(f\"File not found: {data_file_path}\")\n",
    "    \n",
    "    return x_dataset, u_dataset\n",
    "\n",
    "print(config['begin'], config['end'], config['train_data_dir'])\n",
    "x_dataset_train, u_dataset_train = load_evaluation_data(config['begin'], config['end'], config['train_data_dir'])\n",
    "x_dataset_test, u_dataset_test = load_evaluation_data(config['begin'], config['end'], config['test_data_dir'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices = [A_pred, B_pred, C_pred, D_pred]\n",
    "x_data_pred_traj_train, x_data_pca_traj_train, x_data_pca_pred_traj_train = generate_linear_trajectories(y_dataset=x_dataset_train, u_dataset=u_dataset_train, matrices=matrices, std_layer_1=std_layer_1, pca_transformer=pca_layer, std_layer_2=std_layer_2, std_layer_u=std_layer_u)\n",
    "x_data_pred_traj_test, x_data_pca_traj_test, x_data_pca_pred_traj_test = generate_linear_trajectories(y_dataset=x_dataset_test, u_dataset=u_dataset_test, matrices=matrices, std_layer_1=std_layer_1, pca_transformer=pca_layer, std_layer_2=std_layer_2, std_layer_u=std_layer_u)\n",
    "np.save('x_data_pred_traj_train_subspace.npy', x_data_pred_traj_train)\n",
    "np.save('x_data_pred_traj_test_subspace.npy', x_data_pred_traj_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_relative_diff(x_true, x_pred):\n",
    "    row_norm_diff = np.linalg.norm(x_true - x_pred, axis=1, ord=2)\n",
    "    max_norm = np.max(np.linalg.norm(x_true, axis=1, ord=2))\n",
    "    relative_diff = row_norm_diff / max_norm\n",
    "    return relative_diff\n",
    "\n",
    "def calculate_mean_relative_diff_set(x_true_traj, x_pred_traj):\n",
    "    relative_diffs = [calculate_relative_diff(x_true, x_pred) for x_true, x_pred in zip(x_true_traj, x_pred_traj)]\n",
    "    mean_relative_diffs = np.mean(relative_diffs, axis=0)\n",
    "    return mean_relative_diffs\n",
    "\n",
    "def calculate_relative_error(x_true, x_pred):\n",
    "    row_norm_diff = np.linalg.norm(x_true - x_pred, ord='fro')\n",
    "    total_norm_true = np.linalg.norm(x_true, ord='fro')\n",
    "    return row_norm_diff / total_norm_true\n",
    "\n",
    "def calculate_mean_relative_error_set(x_true_traj, x_pred_traj):\n",
    "    relative_errors = [calculate_relative_error(x_true, x_pred) for x_true, x_pred in zip(x_true_traj, x_pred_traj)]\n",
    "    return relative_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate mean relative error\n",
    "# mean_relative_errors_train = calculate_mean_relative_error_set(x_dataset_train, x_data_pred_traj_train)\n",
    "# mean_relative_errors_test = calculate_mean_relative_error_set(x_dataset_test, x_data_pred_traj_test)\n",
    "\n",
    "# # Calculate mean relative diff\n",
    "# mean_relative_diffs_train = calculate_mean_relative_diff_set(x_dataset_train, x_data_pred_traj_train)\n",
    "# mean_relative_diffs_test = calculate_mean_relative_diff_set(x_dataset_test, x_data_pred_traj_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dataset_train_Ip = [arr[:, -1:] for arr in x_dataset_train]\n",
    "x_dataset_test_Ip = [arr[:, -1:] for arr in x_dataset_test]\n",
    "x_dataset_train_Phi = [arr[:, :-1] for arr in x_dataset_train]\n",
    "x_dataset_test_Phi = [arr[:, :-1] for arr in x_dataset_test]\n",
    "\n",
    "x_data_pred_traj_train_Ip = [arr[:, -1:] for arr in x_data_pred_traj_train]\n",
    "x_data_pred_traj_test_Ip = [arr[:, -1:] for arr in x_data_pred_traj_test]\n",
    "x_data_pred_traj_train_Phi = [arr[:, :-1] for arr in x_data_pred_traj_train]\n",
    "x_data_pred_traj_test_Phi = [arr[:, :-1] for arr in x_data_pred_traj_test]\n",
    "\n",
    "np.save(config['save_dir'] + '/x_data_pred_traj_train_subspace_Ip.npy', x_data_pred_traj_train_Ip)\n",
    "np.save(config['save_dir'] + '/x_data_pred_traj_test_subspace_Ip.npy', x_data_pred_traj_test_Ip)\n",
    "np.save(config['save_dir'] + '/x_data_pred_traj_train_subspace_Phi.npy', x_data_pred_traj_train_Phi)\n",
    "np.save(config['save_dir'] + '/x_data_pred_traj_test_subspace_Phi.npy', x_data_pred_traj_test_Phi)\n",
    "\n",
    "mean_relative_errors_train_Ip = calculate_mean_relative_error_set(x_dataset_train_Ip, x_data_pred_traj_train_Ip)\n",
    "mean_relative_errors_test_Ip = calculate_mean_relative_error_set(x_dataset_test_Ip, x_data_pred_traj_test_Ip)\n",
    "mean_relative_errors_train_Phi = calculate_mean_relative_error_set(x_dataset_train_Phi, x_data_pred_traj_train_Phi)\n",
    "mean_relative_errors_test_Phi = calculate_mean_relative_error_set(x_dataset_test_Phi, x_data_pred_traj_test_Phi)\n",
    "\n",
    "mean_relative_diffs_train_Ip = calculate_mean_relative_diff_set(x_dataset_train_Ip, x_data_pred_traj_train_Ip)\n",
    "mean_relative_diffs_test_Ip = calculate_mean_relative_diff_set(x_dataset_test_Ip, x_data_pred_traj_test_Ip)\n",
    "mean_relative_diffs_train_Phi = calculate_mean_relative_diff_set(x_dataset_train_Phi, x_data_pred_traj_train_Phi)\n",
    "mean_relative_diffs_test_Phi = calculate_mean_relative_diff_set(x_dataset_test_Phi, x_data_pred_traj_test_Phi)\n",
    "\n",
    "np.save(config['save_dir'] + '/subspace_mean_relative_errors_train_Ip.npy', mean_relative_errors_train_Ip)\n",
    "np.save(config['save_dir'] + '/subspace_mean_relative_errors_test_Ip.npy', mean_relative_errors_test_Ip)\n",
    "np.save(config['save_dir'] + '/subspace_mean_relative_errors_train_Phi.npy', mean_relative_errors_train_Phi)\n",
    "np.save(config['save_dir'] + '/subspace_mean_relative_errors_test_Phi.npy', mean_relative_errors_test_Phi)\n",
    "np.save(config['save_dir'] + '/subspace_mean_relative_diffs_train_Ip.npy', mean_relative_diffs_train_Ip)\n",
    "np.save(config['save_dir'] + '/subspace_mean_relative_diffs_test_Ip.npy', mean_relative_diffs_test_Ip)\n",
    "np.save(config['save_dir'] + '/subspace_mean_relative_diffs_train_Phi.npy', mean_relative_diffs_train_Phi)\n",
    "np.save(config['save_dir'] + '/subspace_mean_relative_diffs_test_Phi.npy', mean_relative_diffs_test_Phi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('subspace_mean_relative_errors_train.npy', mean_relative_errors_train)\n",
    "# np.save('subspace_mean_relative_errors_test.npy', mean_relative_errors_test)\n",
    "# np.save('subspace_mean_relative_diffs_train.npy', mean_relative_diffs_train)\n",
    "# np.save('subspace_mean_relative_diffs_test.npy', mean_relative_diffs_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # 随机挑选10个样本的行索引\n",
    "# num_samples = 2\n",
    "# indices = np.random.choice(len(x_data_pca_traj_train), num_samples, replace=False)\n",
    "# print(indices)\n",
    "\n",
    "# # 创建4列10行的子图\n",
    "# fig, axs = plt.subplots(num_samples, 4, figsize=(16, 4 * num_samples))\n",
    "\n",
    "# # 循环处理每一个随机选取的样本\n",
    "# for i, idx in enumerate(indices):\n",
    "#     # 获取x_data_pred_traj_train和x_data_pca_traj_train的第idx个样本\n",
    "#     pred_traj = x_data_pred_traj_train[idx]  # 第idx行的数据\n",
    "#     pca_traj = x_data_pca_traj_train[idx]    # 对应的PCA\n",
    "#     print(i)\n",
    "\n",
    "#     # 绘制该样本的不同维度\n",
    "#     for j in range(4):\n",
    "#         axs[i, j].plot(pred_traj[:,j], label='Pred Traj')\n",
    "#         axs[i, j].plot(pca_traj[:, j], label='PCA Traj')\n",
    "#         axs[i, j].set_title(f'Sample {idx} - Dimension {j+1}')\n",
    "#         axs[i, j].legend()\n",
    "#         axs[i, j].grid(True)\n",
    "\n",
    "# # 设置整体布局和展示\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_relative_error(x_data_pca_traj_train[4], x_data_pca_pred_traj_train[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # 随机挑选10个样本的行索引\n",
    "# num_samples = 10\n",
    "# indices = np.random.choice(len(x_data_pca_traj_train), num_samples, replace=False)\n",
    "# print(indices)\n",
    "\n",
    "# # 创建4列10行的子图\n",
    "# fig, axs = plt.subplots(num_samples, 4, figsize=(16, 4 * num_samples))\n",
    "\n",
    "# # 循环处理每一个随机选取的样本\n",
    "# for i, idx in enumerate(indices):\n",
    "#     # 获取x_data_pred_traj_train和x_data_pca_traj_train的第idx个样本\n",
    "#     pred_traj = x_dataset_train[idx]  # 第idx行的数据\n",
    "#     pca_traj = x_data_pred_traj_train[idx]    # 对应的PCA\n",
    "#     print(i)\n",
    "\n",
    "#     # 绘制该样本的不同维度\n",
    "#     for j in range(4):\n",
    "#         axs[i, j].plot(pred_traj[:,j], label='Pred Traj')\n",
    "#         axs[i, j].plot(pca_traj[:, j], label='PCA Traj')\n",
    "#         axs[i, j].set_title(f'Sample {idx} - Dimension {j+1}')\n",
    "#         axs[i, j].legend()\n",
    "#         axs[i, j].grid(True)\n",
    "\n",
    "# # 设置整体布局和展示\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
